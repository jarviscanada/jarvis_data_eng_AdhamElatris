{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e27d0690-2d89-4ac2-b858-f1099f83c6d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------+--------------------+-------+--------------+-------------+-------------------+\n|memid|  surname|firstname|              adress|zipcode|     telephone|recommendedby|           joindate|\n+-----+---------+---------+--------------------+-------+--------------+-------------+-------------------+\n|    0|    GUEST|    GUEST|               GUEST|      0|(000) 000-0000|         null|2012-07-01 00:00:00|\n|    1|    Smith|   Darren|8 Bloomsbury Clos...|   4321|  555-555-5555|         null|2012-07-02 12:02:05|\n|    2|    Smith|    Tracy|8 Bloomsbury Clos...|   4321|  555-555-5555|         null|2012-07-02 12:08:23|\n|    3|   Rownam|      Tim|23 Highway Way, B...|  23423|(844) 693-0723|         null|2012-07-03 09:32:15|\n|    4| Joplette|   Janice|20 Crossing Road,...|    234|(833) 942-4710|            1|2012-07-03 10:25:05|\n|    5|  Butters|   Gerald|1065 Huntingdon A...|  56754|(844) 078-4130|            1|2012-07-09 10:44:09|\n|    6|    Tracy|   Burton|3 Tunisia Drive, ...|  45678|(822) 354-9973|         null|2012-07-15 08:52:55|\n|    7|     Dare|    Nancy|6 Hunting Lodge W...|  10383|(833) 776-4001|            4|2012-07-25 08:59:12|\n|    8|   Boothe|      Tim|3 Bloomsbury Clos...|    234|(811) 433-2547|            3|2012-07-25 16:02:35|\n|    9| Stibbons|   Ponder|5 Dragons Way, Wi...|  87630|(833) 160-3900|            6|2012-07-25 17:09:05|\n|   10|     Owen|  Charles|52 Cheshire Grove...|  28563|(855) 542-5251|            1|2012-08-03 19:42:37|\n|   11|    Jones|    David|976 Gnats Close, ...|  33862|(844) 536-8036|            4|2012-08-06 16:32:55|\n|   12|    Baker|     Anne|55 Powdery Street...|  80743|  844-076-5141|            9|2012-08-10 14:23:22|\n|   13|  Farrell|   Jemima|103 Firth Avenue,...|  57392|(855) 016-0163|         null|2012-08-10 14:28:01|\n|   14|    Smith|     Jack|252 Binkington Wa...|  69302|(822) 163-3254|            1|2012-08-10 16:22:05|\n|   15|    Bader| Florence|264 Ursula Drive,...|  84923|(833) 499-3527|            9|2012-08-10 17:52:03|\n|   16|    Baker|  Timothy|329 James Street,...|  58393|  833-941-0824|           13|2012-08-15 10:34:25|\n|   17|   Pinker|    David|5 Impreza Road, B...|  65332|  811 409-6734|           13|2012-08-16 11:32:47|\n|   20|  Genting|  Matthew|4 Nunnington Plac...|  52365|(811) 972-1377|            5|2012-08-19 14:55:55|\n|   21|Mackenzie|     Anna|64 Perkington Lan...|  64577|(822) 661-2898|            1|2012-08-26 09:32:05|\n+-----+---------+---------+--------------------+-------+--------------+-------------+-------------------+\nonly showing top 20 rows\n\n+------+-----+-----+-------------------+-----+\n|bookid|facid|memid|          starttime|slots|\n+------+-----+-----+-------------------+-----+\n|     0|    3|    1|2012-07-03 11:00:00|    2|\n|     1|    4|    1|2012-07-03 08:00:00|    2|\n|     2|    6|    0|2012-07-03 18:00:00|    2|\n|     3|    7|    1|2012-07-03 19:00:00|    2|\n|     4|    8|    1|2012-07-03 10:00:00|    1|\n|     5|    8|    1|2012-07-03 15:00:00|    1|\n|     6|    0|    2|2012-07-04 09:00:00|    3|\n|     7|    0|    2|2012-07-04 15:00:00|    3|\n|     8|    4|    3|2012-07-04 13:30:00|    2|\n|     9|    4|    0|2012-07-04 15:00:00|    2|\n|    10|    4|    0|2012-07-04 17:30:00|    2|\n|    11|    6|    0|2012-07-04 12:30:00|    2|\n|    12|    6|    0|2012-07-04 14:00:00|    2|\n|    13|    6|    1|2012-07-04 15:30:00|    2|\n|    14|    7|    2|2012-07-04 14:00:00|    2|\n|    15|    8|    2|2012-07-04 12:00:00|    1|\n|    16|    8|    3|2012-07-04 18:00:00|    1|\n|    17|    1|    0|2012-07-05 17:30:00|    3|\n|    18|    2|    1|2012-07-05 09:30:00|    3|\n|    19|    3|    3|2012-07-05 09:00:00|    2|\n+------+-----+-----+-------------------+-----+\nonly showing top 20 rows\n\n+-----+---------------+----------+---------+-------------+------------------+\n|facid|           name|membercost|guestcost|initialoutlay|monthlymaintenance|\n+-----+---------------+----------+---------+-------------+------------------+\n|    0| Tennis Court 1|       5.0|     25.0|      10000.0|             200.0|\n|    1| Tennis Court 2|       5.0|     25.0|       8000.0|             200.0|\n|    2|Badminton Court|       0.0|     15.5|       4000.0|              50.0|\n|    3|   Table Tennis|       0.0|      5.0|        320.0|              10.0|\n|    4| Massage Room 1|      35.0|     80.0|       4000.0|            3000.0|\n|    5| Massage Room 2|      35.0|     80.0|       4000.0|            3000.0|\n|    6|   Squash Court|       3.5|     17.5|       5000.0|              80.0|\n|    7|  Snooker Table|       0.0|      5.0|        450.0|              15.0|\n|    8|     Pool Table|       0.0|      5.0|        400.0|              15.0|\n+-----+---------------+----------+---------+-------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "members_df = spark.sql(\"SELECT * FROM members\")\n",
    "\n",
    "bookings_df = spark.sql(\"SELECT * FROM bookings\")\n",
    "\n",
    "facilities_df = spark.sql(\"SELECT * FROM facilities\")\n",
    "\n",
    "members_df.show()\n",
    "\n",
    "bookings_df.show()\n",
    "\n",
    "facilities_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51a95c74-98b6-4ad2-bdfc-67a2a2b7086f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebfd9bca-5253-44a5-85ce-80f5126ce9fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n|          starttime|\n+-------------------+\n|2012-09-18 09:00:00|\n|2012-09-18 17:30:00|\n|2012-09-18 13:30:00|\n|2012-09-18 20:00:00|\n|2012-09-19 09:30:00|\n|2012-09-19 15:00:00|\n|2012-09-19 12:00:00|\n|2012-09-20 15:30:00|\n|2012-09-20 11:30:00|\n|2012-09-20 14:00:00|\n|2012-09-21 10:30:00|\n|2012-09-21 14:00:00|\n|2012-09-22 08:30:00|\n|2012-09-22 17:00:00|\n|2012-09-23 08:30:00|\n|2012-09-23 17:30:00|\n|2012-09-23 19:00:00|\n|2012-09-24 08:00:00|\n|2012-09-24 16:30:00|\n|2012-09-24 12:30:00|\n+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 1 How can you produce a list of the start times for bookings by members named 'David Farrell'?\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = bookings_df.join(members_df, \"memid\").filter((col(\"firstname\") == \"David\") & (col(\"surname\") == \"Farrell\")).select(\"starttime\")\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15516bf4-ea82-47b6-87ef-d6a028ef835b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+\n|starttime          |name          |\n+-------------------+--------------+\n|2012-09-21 08:00:00|Tennis Court 1|\n|2012-09-21 08:00:00|Tennis Court 2|\n|2012-09-21 09:30:00|Tennis Court 1|\n|2012-09-21 10:00:00|Tennis Court 2|\n|2012-09-21 11:30:00|Tennis Court 2|\n|2012-09-21 12:00:00|Tennis Court 1|\n|2012-09-21 13:30:00|Tennis Court 1|\n|2012-09-21 14:00:00|Tennis Court 2|\n|2012-09-21 15:30:00|Tennis Court 1|\n|2012-09-21 16:00:00|Tennis Court 2|\n|2012-09-21 17:00:00|Tennis Court 1|\n|2012-09-21 18:00:00|Tennis Court 2|\n+-------------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2 How can you produce a list of the start times for bookings for tennis courts, for the date '2012-09-21'? Return a list of start time and facility name pairings, ordered by the time.\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = (\n",
    "    bookings_df.join(facilities_df, \"facid\")\n",
    "    .filter((col(\"name\").isin(\"Tennis Court 1\", \"Tennis Court 2\")) & \n",
    "            (col(\"starttime\") >= \"2012-09-21\") & \n",
    "            (col(\"starttime\") < \"2012-09-22\")) \n",
    "    .select(\"starttime\", \"name\")\n",
    "    .orderBy(\"starttime\")\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8645689-38d5-466f-a65c-181368f21f7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+--------+\n| memfname| memsname| recfname|recsname|\n+---------+---------+---------+--------+\n| Florence|    Bader|   Ponder|Stibbons|\n|     Anne|    Baker|   Ponder|Stibbons|\n|  Timothy|    Baker|   Jemima| Farrell|\n|      Tim|   Boothe|      Tim|  Rownam|\n|   Gerald|  Butters|   Darren|   Smith|\n|     Joan|   Coplin|  Timothy|   Baker|\n|    Erica|  Crumpet|    Tracy|   Smith|\n|    Nancy|     Dare|   Janice|Joplette|\n|    David|  Farrell|     null|    null|\n|   Jemima|  Farrell|     null|    null|\n|    GUEST|    GUEST|     null|    null|\n|  Matthew|  Genting|   Gerald| Butters|\n|     John|     Hunt|Millicent| Purview|\n|    David|    Jones|   Janice|Joplette|\n|  Douglas|    Jones|    David|   Jones|\n|   Janice| Joplette|   Darren|   Smith|\n|     Anna|Mackenzie|   Darren|   Smith|\n|  Charles|     Owen|   Darren|   Smith|\n|    David|   Pinker|   Jemima| Farrell|\n|Millicent|  Purview|    Tracy|   Smith|\n+---------+---------+---------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 3 How can you output a list of all members, including the individual who recommended them (if any)? Ensure that results are ordered by (surname, firstname).\n",
    "\n",
    "from pyspark.sql import functions\n",
    "\n",
    "df = (\n",
    "    members_df.alias(\"mems\") \n",
    "    .join(members_df.alias(\"recs\"), col(\"mems.recommendedby\") == col(\"recs.memid\"), \"left\") \n",
    "    .select(\n",
    "        col(\"mems.firstname\").alias(\"memfname\"), \n",
    "        col(\"mems.surname\").alias(\"memsname\"), \n",
    "        col(\"recs.firstname\").alias(\"recfname\"), \n",
    "        col(\"recs.surname\").alias(\"recsname\")\n",
    "    ) \n",
    "    .orderBy(col(\"memsname\"), col(\"memfname\"))\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93dff3fa-4034-48fb-8151-a128bd6b165e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n|        member|      facility|\n+--------------+--------------+\n|    Anne Baker|Tennis Court 1|\n|    Anne Baker|Tennis Court 2|\n|  Burton Tracy|Tennis Court 1|\n|  Burton Tracy|Tennis Court 2|\n|  Charles Owen|Tennis Court 1|\n|  Charles Owen|Tennis Court 2|\n|  Darren Smith|Tennis Court 2|\n| David Farrell|Tennis Court 1|\n| David Farrell|Tennis Court 2|\n|   David Jones|Tennis Court 1|\n|   David Jones|Tennis Court 2|\n|  David Pinker|Tennis Court 1|\n| Douglas Jones|Tennis Court 1|\n| Erica Crumpet|Tennis Court 1|\n|Florence Bader|Tennis Court 1|\n|Florence Bader|Tennis Court 2|\n|   GUEST GUEST|Tennis Court 1|\n|   GUEST GUEST|Tennis Court 2|\n|Gerald Butters|Tennis Court 1|\n|Gerald Butters|Tennis Court 2|\n+--------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, concat_ws\n",
    "\n",
    "# 4 How can you produce a list of all members who have used a tennis court? Include in your output the name of the court, and the name of the member formatted as a single column. Ensure no duplicate data, and order by the member name followed by the facility name.\n",
    "\n",
    "df = (\n",
    "    bookings_df\n",
    "    .join(members_df, \"memid\")  \n",
    "    .join(facilities_df, \"facid\") \n",
    "    .filter(col(\"name\").isin(\"Tennis Court 1\", \"Tennis Court 2\"))  \n",
    "    .select(\n",
    "        concat_ws(\" \", col(\"firstname\"), col(\"surname\")).alias(\"member\"),\n",
    "        col(\"name\").alias(\"facility\")\n",
    "    )\n",
    "    .distinct() \n",
    "    .orderBy(\"member\", \"facility\")  \n",
    ")\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5101edd-5fc7-4713-8746-3fe324e56ecc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n|              member|    recommender|\n+--------------------+---------------+\n|      Anna Mackenzie|   Darren Smith|\n|          Anne Baker|Ponder Stibbons|\n|        Burton Tracy|               |\n|        Charles Owen|   Darren Smith|\n|        Darren Smith|               |\n|       David Farrell|               |\n|         David Jones|Janice Joplette|\n|        David Pinker| Jemima Farrell|\n|       Douglas Jones|    David Jones|\n|       Erica Crumpet|    Tracy Smith|\n|      Florence Bader|Ponder Stibbons|\n|         GUEST GUEST|               |\n|      Gerald Butters|   Darren Smith|\n|    Henrietta Rumney|Matthew Genting|\n|Henry Worthington...|    Tracy Smith|\n| Hyacinth Tupperware|               |\n|          Jack Smith|   Darren Smith|\n|     Janice Joplette|   Darren Smith|\n|      Jemima Farrell|               |\n|         Joan Coplin|  Timothy Baker|\n+--------------------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 5 How can you output a list of all members, including the individual who recommended them (if any), without using any joins? Ensure that there are no duplicates in the list, and that each firstname + surname pairing is formatted as a column and ordered.\n",
    "\n",
    "from pyspark.sql.functions import col, concat_ws\n",
    "\n",
    "df = (\n",
    "    members_df.alias(\"mems\")\n",
    "    .join(\n",
    "        members_df.alias(\"recs\"),\n",
    "        col(\"mems.recommendedby\") == col(\"recs.memid\"),\n",
    "        \"left\" \n",
    "    )\n",
    "    .select(\n",
    "        concat_ws(\" \", col(\"mems.firstname\"), col(\"mems.surname\")).alias(\"member\"),\n",
    "        concat_ws(\" \", col(\"recs.firstname\"), col(\"recs.surname\")).alias(\"recommender\")\n",
    "    )\n",
    "    .distinct() \n",
    "    .orderBy(\"member\") \n",
    ")\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8fbdb36-84ea-440f-8249-f966fbf005b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n|recommendedby|count|\n+-------------+-----+\n|            1|    5|\n|            2|    3|\n|            3|    1|\n|            4|    2|\n|            5|    1|\n|            6|    1|\n|            9|    2|\n|           11|    1|\n|           13|    2|\n|           15|    1|\n|           16|    1|\n|           20|    1|\n|           30|    1|\n+-------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 6 Produce a count of the number of recommendations each member has made. Order by member ID.\n",
    "\n",
    "from pyspark.sql import functions\n",
    "\n",
    "df = (\n",
    "    members_df\n",
    "    .filter(col(\"recommendedby\").isNotNull()) \n",
    "    .groupBy(\"recommendedby\") \n",
    "    .agg(functions.count(\"*\").alias(\"count\"))\n",
    "    .orderBy(\"recommendedby\") \n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7ac5b00-4433-4d4f-b7b4-722aa39d57a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n|facid|Total Slot|\n+-----+----------+\n|    0|      1320|\n|    1|      1278|\n|    2|      1209|\n|    3|       830|\n|    4|      1404|\n|    5|       228|\n|    6|      1104|\n|    7|       908|\n|    8|       911|\n+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 7 Produce a list of the total number of slots booked per facility. For now, just produce an output table consisting of facility id and slots, sorted by facility id.\n",
    "from pyspark.sql import functions\n",
    "\n",
    "df = (\n",
    "    bookings_df\n",
    "    .select(\"facid\", \"slots\")\n",
    "    .groupBy(\"facid\")\n",
    "    .agg(sum(\"slots\").alias(\"Total Slot\"))\n",
    "    .orderBy(\"facid\")\n",
    "    )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ad65107-5352-4ba0-994b-621ec5829104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n|facid|Total Slots|\n+-----+-----------+\n|    5|        122|\n|    3|        422|\n|    7|        426|\n|    8|        471|\n|    6|        540|\n|    2|        570|\n|    1|        588|\n|    0|        591|\n|    4|        648|\n+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 8 Produce a list of the total number of slots booked per facility in the month of September 2012. Produce an output table consisting of facility id and slots, sorted by the number of slots.\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "df= (\n",
    "    bookings_df\n",
    "    .select(\"facid\", \"slots\")\n",
    "    .filter((f.col(\"starttime\") >= '2012-09-01') & (f.col(\"starttime\") < '2012-10-01'))\n",
    "    .groupBy(\"facid\")\n",
    "    .agg(sum(\"slots\").alias(\"Total Slots\"))\n",
    "    .orderBy(\"Total Slots\")\n",
    ")\n",
    "\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e4342f8-26ab-458c-a52a-2161cf36c68b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+\n|facid|month|Total Slots|\n+-----+-----+-----------+\n|    0|    7|        270|\n|    0|    8|        459|\n|    0|    9|        591|\n|    1|    7|        207|\n|    1|    8|        483|\n|    1|    9|        588|\n|    2|    7|        180|\n|    2|    8|        459|\n|    2|    9|        570|\n|    3|    7|        104|\n|    3|    8|        304|\n|    3|    9|        422|\n|    4|    7|        264|\n|    4|    8|        492|\n|    4|    9|        648|\n|    5|    7|         24|\n|    5|    8|         82|\n|    5|    9|        122|\n|    6|    7|        164|\n|    6|    8|        400|\n+-----+-----+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 9 Produce a list of the total number of slots booked per facility per month in the year of 2012. Produce an output table consisting of facility id and slots, sorted by the id and month.\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "df = (\n",
    "    bookings_df\n",
    "    .filter(f.year(\"starttime\") == 2012)  \n",
    "    .select(\n",
    "        \"facid\",\n",
    "        f.month(\"starttime\").alias(\"month\"),  \n",
    "        \"slots\"\n",
    "    )\n",
    "    .groupBy(\"facid\", \"month\")  \n",
    "    .agg(f.sum(\"slots\").alias(\"Total Slots\"))  \n",
    "    .orderBy(\"facid\", \"month\")  \n",
    ")\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2206cd55-ba66-4f0c-a751-b7d10068d66e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n|count(memid)|\n+------------+\n|          30|\n+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 10 Find the total number of members (including guests) who have made at least one booking.\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "df = (\n",
    "    bookings_df\n",
    "    .select(\"memid\")\n",
    "    .dropDuplicates()\n",
    "    .agg(f.count(\"memid\"))\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc05840a-a5c9-49cc-92be-a72ca7f238ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+-------------------+\n|  surname|firstname|memid|          starttime|\n+---------+---------+-----+-------------------+\n|    GUEST|    GUEST|    0|2012-09-01 08:00:00|\n|    Smith|   Darren|    1|2012-09-01 09:00:00|\n|    Smith|    Tracy|    2|2012-09-01 11:30:00|\n|   Rownam|      Tim|    3|2012-09-01 16:00:00|\n| Joplette|   Janice|    4|2012-09-01 15:00:00|\n|  Butters|   Gerald|    5|2012-09-02 12:30:00|\n|    Tracy|   Burton|    6|2012-09-01 15:00:00|\n|     Dare|    Nancy|    7|2012-09-01 12:30:00|\n|   Boothe|      Tim|    8|2012-09-01 08:30:00|\n| Stibbons|   Ponder|    9|2012-09-01 11:00:00|\n|     Owen|  Charles|   10|2012-09-01 11:00:00|\n|    Jones|    David|   11|2012-09-01 09:30:00|\n|    Baker|     Anne|   12|2012-09-01 14:30:00|\n|  Farrell|   Jemima|   13|2012-09-01 09:30:00|\n|    Smith|     Jack|   14|2012-09-01 11:00:00|\n|    Bader| Florence|   15|2012-09-01 10:30:00|\n|    Baker|  Timothy|   16|2012-09-01 15:00:00|\n|   Pinker|    David|   17|2012-09-01 08:30:00|\n|  Genting|  Matthew|   20|2012-09-01 18:00:00|\n|Mackenzie|     Anna|   21|2012-09-01 08:30:00|\n+---------+---------+-----+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 11 Produce a list of each member name, id, and their first booking after September 1st 2012. Order by member ID.\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "df = (\n",
    "     bookings_df\n",
    "    .join(members_df, \"memid\")  \n",
    "    .filter(f.col(\"starttime\") >= '2012-09-01') \n",
    "    .groupBy(\"surname\", \"firstname\", \"memid\")  \n",
    "    .agg(f.min(\"starttime\").alias(\"starttime\")) \n",
    "\n",
    "    .orderBy(\"memid\")  \n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4ef0131-3f26-41bc-b975-fdd29ee285ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n|            name|\n+----------------+\n|    GUEST, GUEST|\n|   Smith, Darren|\n|    Smith, Tracy|\n|     Rownam, Tim|\n|Joplette, Janice|\n| Butters, Gerald|\n|   Tracy, Burton|\n|     Dare, Nancy|\n|     Boothe, Tim|\n|Stibbons, Ponder|\n|   Owen, Charles|\n|    Jones, David|\n|     Baker, Anne|\n| Farrell, Jemima|\n|     Smith, Jack|\n| Bader, Florence|\n|  Baker, Timothy|\n|   Pinker, David|\n|Genting, Matthew|\n| Mackenzie, Anna|\n+----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 12 Output the names of all members, formatted as 'Surname, Firstname'\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "df = (\n",
    "    members_df\n",
    "    .select(f.concat_ws(\", \", f.col(\"surname\"), f.col(\"firstname\")).alias(\"name\"))\n",
    ")\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef221ad9-9bf2-4510-8106-f3ded844367d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+----------+---------+-------------+------------------+\n|facid|          name|membercost|guestcost|initialoutlay|monthlymaintenance|\n+-----+--------------+----------+---------+-------------+------------------+\n|    0|Tennis Court 1|       5.0|     25.0|      10000.0|             200.0|\n|    1|Tennis Court 2|       5.0|     25.0|       8000.0|             200.0|\n+-----+--------------+----------+---------+-------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 13 Perform a case-insensitive search to find all facilities whose name begins with 'tennis'. Retrieve all columns.\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "df = (\n",
    "    facilities_df\n",
    "    .select(\"*\")\n",
    "    .filter(f.upper(col(\"name\")).ilike('TENNIS%')) \n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ae91600-0302-45ac-8dfa-bac7c38e22f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n|memid|     telephone|\n+-----+--------------+\n|    0|(000) 000-0000|\n|    3|(844) 693-0723|\n|    4|(833) 942-4710|\n|    5|(844) 078-4130|\n|    6|(822) 354-9973|\n|    7|(833) 776-4001|\n|    8|(811) 433-2547|\n|    9|(833) 160-3900|\n|   10|(855) 542-5251|\n|   11|(844) 536-8036|\n|   13|(855) 016-0163|\n|   14|(822) 163-3254|\n|   15|(833) 499-3527|\n|   20|(811) 972-1377|\n|   21|(822) 661-2898|\n|   22|(822) 499-2232|\n|   24|(822) 413-1470|\n|   27|(822) 989-8876|\n|   28|(855) 755-9876|\n|   29|(855) 894-3758|\n+-----+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 14 You've noticed that the club's member table has telephone numbers with very inconsistent formatting. You'd like to find all the telephone numbers that contain parentheses, returning the member ID and telephone number sorted by member ID.\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "df = (\n",
    "    members_df\n",
    "    .select(\"memid\", \"telephone\")\n",
    "    .filter(f.col(\"telephone\").rlike(\"[()]\"))\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a865217-c58b-47cd-829b-5b31f5c37a8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n|letter|count|\n+------+-----+\n|     B|    5|\n|     C|    2|\n|     D|    1|\n|     F|    2|\n|     G|    2|\n|     H|    1|\n|     J|    3|\n|     M|    1|\n|     O|    1|\n|     P|    2|\n|     R|    2|\n|     S|    6|\n|     T|    2|\n|     W|    1|\n+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 15 You'd like to produce a count of how many members you have whose surname starts with each letter of the alphabet. Sort by the letter, and don't worry about printing out a letter if the count is 0.\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "df = (\n",
    "    members_df\n",
    "    .select(f.col(\"surname\").substr(1,1).alias(\"letter\"))\n",
    "    .groupBy(\"letter\")\n",
    "    .agg(f.count(\"*\").alias(\"count\"))\n",
    "    .orderBy(\"letter\")\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa51973d-4f73-40fb-8522-0cac9414864b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n|                 ts|\n+-------------------+\n|2012-10-01 00:00:00|\n|2012-10-02 00:00:00|\n|2012-10-03 00:00:00|\n|2012-10-04 00:00:00|\n|2012-10-05 00:00:00|\n|2012-10-06 00:00:00|\n|2012-10-07 00:00:00|\n|2012-10-08 00:00:00|\n|2012-10-09 00:00:00|\n|2012-10-10 00:00:00|\n|2012-10-11 00:00:00|\n|2012-10-12 00:00:00|\n|2012-10-13 00:00:00|\n|2012-10-14 00:00:00|\n|2012-10-15 00:00:00|\n|2012-10-16 00:00:00|\n|2012-10-17 00:00:00|\n|2012-10-18 00:00:00|\n|2012-10-19 00:00:00|\n|2012-10-20 00:00:00|\n+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 16 Produce a list of all the dates in October 2012. They can be output as a timestamp (with time set to midnight) or a date.\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define start and end dates\n",
    "start_date = datetime(2012, 10, 1)\n",
    "end_date = datetime(2012, 10, 31)\n",
    "\n",
    "df = (\n",
    "    spark.range(1)  # Dummy row\n",
    "    .select(\n",
    "        f.explode(\n",
    "            f.sequence(\n",
    "                f.lit(start_date).cast(TimestampType()), \n",
    "                f.lit(end_date).cast(TimestampType()), \n",
    "                f.expr(\"INTERVAL 1 DAY\")\n",
    "            )\n",
    "        ).alias(\"ts\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b66d14-a316-4fa9-a40b-3f5d8d117611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n|              month|count|\n+-------------------+-----+\n|2012-07-01 00:00:00|  658|\n|2012-08-01 00:00:00| 1472|\n|2012-09-01 00:00:00| 1913|\n|2013-01-01 00:00:00|    1|\n+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 17 Return a count of bookings for each month, sorted by month\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "df = (\n",
    "    bookings_df\n",
    "    .select(\"starttime\")\n",
    "    .groupBy(f.date_trunc(\"month\", f.col(\"starttime\")).alias(\"month\"))\n",
    "    .agg(f.count(\"*\").alias(\"count\")) \n",
    "    .orderBy(\"month\") \n",
    ")\n",
    "\n",
    "df.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SQL questions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}